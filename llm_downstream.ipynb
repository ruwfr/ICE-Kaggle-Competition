{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Artificial Textclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach uses a LLM for text encoding and learns a downstream task for detection of artificial generated texts based on this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import dataprocessing.dataset as ds\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hardware usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "num_epochs = 5\n",
    "num_classes = 2\n",
    "batch_size = 20\n",
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    train_set_1 = pd.read_csv(\"./data/train_drcat_04.csv\")\n",
    "    #using only data with label == 1\n",
    "    #train_set_1 = train_set_1[train_set_1[\"label\"]==1]\n",
    "    train_set_1 = train_set_1[[\"text\",\"label\"]]\n",
    "    train_set_1['text'] = train_set_1['text'].str.replace('\\n', '')\n",
    "\n",
    "    train_set_2 = pd.read_csv(\"./data/daigt_external_dataset.csv\", sep=',')\n",
    "    train_set_2 = train_set_2.rename(columns={'generated': 'label'})\n",
    "    train_set_2 = train_set_2[[\"source_text\"]]\n",
    "    train_set_2.columns = [\"text\"]\n",
    "    train_set_2['text'] = train_set_2['text'].str.replace('\\n', '')\n",
    "    train_set_2[\"label\"] = 1\n",
    "\n",
    "    train_set_3 = pd.read_csv(\"./data/train_essays_RDizzl3_seven_v1.csv\")\n",
    "\n",
    "    train_set = pd.concat([train_set_1,train_set_2,train_set_3])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_set[\"text\"],train_set[\"label\"],test_size=0.2)\n",
    "\n",
    "    data_train = []\n",
    "    data_val = []\n",
    "    max_sequence_length = 0\n",
    "\n",
    "    for ii in range(len(X_train)):\n",
    "        data_train.append({'text': X_train.values[ii], 'label': y_train.values[ii]})\n",
    "        if len(X_train.values[ii]) > max_sequence_length: max_sequence_length=len(X_train.values[ii])\n",
    "    for ii in range(len(X_val)):\n",
    "        data_val.append({'text': X_val.values[ii], 'label': y_val.values[ii]})\n",
    "        if len(X_val.values[ii]) > max_sequence_length: max_sequence_length=len(X_val.values[ii])\n",
    "\n",
    "    print(f'Number of Training Data: {len(y_train)}, Number of Validation Data: {len(y_val)}')\n",
    "\n",
    "    return data_train, data_val, max_sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data, tokenizer, max_length):\n",
    "    \n",
    "    data_list = []  \n",
    "\n",
    "    for ii in tqdm(range(len(data)), desc=f'Tokenize'):\n",
    "        text = data[ii]['text']\n",
    "        label = data[ii]['label']\n",
    "\n",
    "        # Tokenize the text using the GPT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length' if max_length else 'longest',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        data_list.append({'text': tokenized_text, 'label': label})\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, max_sequence_length = load_dataset()\n",
    "\n",
    "if max_sequence_length > 50: max_sequence_length = 50\n",
    "print(f'Maximum Sequence Lenght: {max_sequence_length}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_train = tokenize_function(data_train, tokenizer, max_sequence_length)\n",
    "data_val = tokenize_function(data_val, tokenizer, max_sequence_length)\n",
    "\n",
    "train_custom_dataset = ds.CustomDataset(data_train)\n",
    "val_custom_dataset = ds.CustomDataset(data_val)\n",
    "\n",
    "train_data_loader = DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_custom_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        # Calculate pairwise distances\n",
    "        pairwise_distances = torch.cdist(embeddings, embeddings)\n",
    "        \n",
    "        # Expand labels to compare each pair\n",
    "        expanded_labels = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        \n",
    "        # Calculate loss based on pairwise distances and labels\n",
    "        loss_matrix = expanded_labels.float() * torch.pow(pairwise_distances, 2) + \\\n",
    "                      (~expanded_labels).float() * torch.pow(torch.clamp(self.margin - pairwise_distances, min=0.0), 2)\n",
    "        \n",
    "        # Ignore diagonal elements\n",
    "        mask = ~torch.eye(loss_matrix.size(0), dtype=bool, device=loss_matrix.device)\n",
    "        loss_contrastive = torch.masked_select(loss_matrix, mask).mean()\n",
    "        \n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2Model.from_pretrained(model_name)\n",
    "# resize model embedding to match new tokenizer\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "# fix model padding token id\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "base_model.to(device)\n",
    "\n",
    "# loss function\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(base_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG-Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGboost():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.xgb_cl = xgb.XGBClassifier()\n",
    "        self.preds = None\n",
    "        self.acc_score = None\n",
    "\n",
    "    def fit(self, data_loader, base_model, device):\n",
    "\n",
    "        encoded_data = []\n",
    "        labels_data = []\n",
    "        fit_pbar = tqdm(data_loader, desc=\"Fit XG-Boost\", leave=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_fit in fit_pbar:\n",
    "                inputs_fit, labels_fit = batch_fit\n",
    "                inputs_fit.to(device)\n",
    "                labels_fit = labels_fit.type(torch.FloatTensor)\n",
    "                labels_fit = labels_fit.to(device)\n",
    "                outputs_fit = base_model(**inputs_fit)\n",
    "                last_hidden_states_fit = outputs_fit.last_hidden_state\n",
    "                sequence_embedding_fit = last_hidden_states_fit[:,0].mean(dim=1)\n",
    "                encoded_data.append(sequence_embedding_fit.clone().detach().cpu().numpy())\n",
    "                labels_data.append(labels_fit.clone().detach().cpu().numpy())\n",
    "\n",
    "        encoded_data = np.vstack(encoded_data).reshape((-1, base_model.config.hidden_size))\n",
    "        labels_data = np.vstack(labels_data).reshape((-1,1)).astype('int8')\n",
    "\n",
    "        self.xgb_cl.fit(encoded_data, labels_data)\n",
    "\n",
    "\n",
    "    def predict(self, data_loader, base_model, device):\n",
    "\n",
    "        encoded_data = []\n",
    "        labels_data = []\n",
    "        pred_pbar = tqdm(data_loader, desc=\"Predict XG-Boost\", leave=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_pred in pred_pbar:\n",
    "                inputs_pred, labels_pred = batch_pred\n",
    "                inputs_pred.to(device)\n",
    "                labels_pred = labels_pred.type(torch.FloatTensor)\n",
    "                labels_pred = labels_pred.to(device)\n",
    "                outputs_pred = base_model(**inputs_pred)\n",
    "                last_hidden_states_pred = outputs_pred.last_hidden_state\n",
    "                sequence_embedding_pred = last_hidden_states_pred[:,0].mean(dim=1) \n",
    "                encoded_data.append(sequence_embedding_pred.clone().detach().cpu().numpy())\n",
    "                labels_data.append(labels_pred.clone().detach().cpu().numpy())\n",
    "\n",
    "        encoded_data = np.vstack(encoded_data).reshape((-1, base_model.config.hidden_size))\n",
    "        labels_data = np.vstack(labels_data).reshape((-1,1)).astype('int8')\n",
    "\n",
    "        self.preds = self.xgb_cl.predict(encoded_data)\n",
    "        self.acc_score = accuracy_score(labels_data, self.preds)\n",
    "\n",
    "        return self.preds\n",
    "    \n",
    "    def get_acc_score(self):\n",
    "\n",
    "        return self.acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict XG-Boost\n",
    "xgb_cl = XGboost()\n",
    "xgb_cl.fit(train_data_loader, base_model, device)\n",
    "xgb_cl.predict(val_data_loader, base_model, device)\n",
    "acc_pretrained = xgb_cl.get_acc_score()\n",
    "print(f'Pretraining Accuracy XG-Boost Classifier: {acc_pretrained}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(data_loader, model, device, title):\n",
    "\n",
    "    encoded_data = []\n",
    "    labels_data = []\n",
    "    test_pbar = tqdm(data_loader, desc=\"Visualize Embedding\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_test in test_pbar:\n",
    "            inputs_test, labels_test = batch_test\n",
    "            inputs_test.to(device)\n",
    "            labels_test = labels_test.type(torch.FloatTensor)\n",
    "            labels_test = labels_test.to(device)\n",
    "            outputs_test = model(**inputs_test)\n",
    "            last_hidden_states_test = outputs_test.last_hidden_state\n",
    "            sequence_embedding_test = last_hidden_states_test[:,0].mean(dim=1) \n",
    "            encoded_data.append(sequence_embedding_test.clone().detach().cpu().numpy())\n",
    "            labels_data.append(labels_test.clone().detach().cpu().numpy())\n",
    "\n",
    "    encoded_data = np.vstack(encoded_data).reshape((-1, model.config.hidden_size))\n",
    "    labels_data = np.vstack(labels_data).reshape((-1,1)).astype('int8')\n",
    "\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3)\n",
    "    proj_data = tsne.fit_transform(encoded_data)\n",
    "\n",
    "    lda_result_df = pd.DataFrame({'dim_1': proj_data[:,0], 'dim_2': proj_data[:,1], 'label': labels_data[:,0]})\n",
    "    fig, ax = plt.subplots()\n",
    "    s = ax.scatter(lda_result_df['dim_1'], lda_result_df['dim_2'], c=labels_data, s=120)\n",
    "    ax.set_xlabel('dim_1')\n",
    "    ax.set_ylabel('dim_2')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(s.legend_elements()[0],['0','1'], loc=2)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot base model embedding representation\n",
    "plot_embedding(val_data_loader, base_model, device, 'Preprained Embedding Representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop contrastive learning\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    train_pbar = tqdm(train_data_loader, desc=f'Epoch: {epoch}/{num_epochs}', leave=True)\n",
    "    for batch_train in train_pbar:\n",
    "        inputs_train, labels_train = batch_train\n",
    "        inputs_train.to(device)\n",
    "        labels_train = labels_train.type(torch.FloatTensor)\n",
    "        labels_train = labels_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_train = base_model(**inputs_train)\n",
    "        last_hidden_states_train = outputs_train.last_hidden_state\n",
    "        sequence_embedding_train = last_hidden_states_train[:,0].mean(dim=1) \n",
    "\n",
    "        # Compute contrastive loss using representations\n",
    "        contrastive_loss_value_train = contrastive_loss.forward(sequence_embedding_train, labels_train)/batch_size\n",
    "\n",
    "        loss_train_epoch.append(contrastive_loss_value_train.item())\n",
    "        train_pbar.set_postfix({'train_loss': np.mean(loss_train_epoch)})\n",
    "\n",
    "        # Backward pass\n",
    "        contrastive_loss_value_train.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_val in val_data_loader:\n",
    "            inputs_val, labels_val = batch_val\n",
    "            inputs_val.to(device)\n",
    "            labels_val = labels_val.type(torch.FloatTensor)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_val = base_model(**inputs_val)\n",
    "            last_hidden_states_val = outputs_val.last_hidden_state\n",
    "            sequence_embedding_val = last_hidden_states_val[:,0].mean(dim=1) \n",
    "\n",
    "            # Compute contrastive loss using representations\n",
    "            contrastive_loss_value_val = contrastive_loss.forward(sequence_embedding_val, labels_val)/batch_size\n",
    "\n",
    "            loss_val_epoch.append(contrastive_loss_value_val.item())\n",
    "\n",
    "    print(f'LOSS train: {np.mean(loss_train_epoch)} valid: {np.mean(loss_val_epoch)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict XG-Boost\n",
    "xgb_cl = XGboost()\n",
    "xgb_cl.fit(train_data_loader, base_model, device)\n",
    "xgb_cl.predict(val_data_loader, base_model, device)\n",
    "acc_finetuned = xgb_cl.get_acc_score()\n",
    "print(f'Finetuned Accuracy XG-Boost Classifier: {acc_finetuned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(val_data_loader, base_model, device, 'Finetuned Embedding Representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Pretraining: {acc_pretrained} Finetuned: {acc_finetuned}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiTextGenEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
