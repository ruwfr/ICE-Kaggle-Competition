{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T05:02:18.110470Z","iopub.status.busy":"2023-11-27T05:02:18.109937Z","iopub.status.idle":"2023-11-27T05:02:18.132224Z","shell.execute_reply":"2023-11-27T05:02:18.130837Z","shell.execute_reply.started":"2023-11-27T05:02:18.110424Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Git\\kaggle\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import regex as re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from xgboost import XGBClassifier\n","from imblearn.under_sampling import RandomUnderSampler\n","import random\n","from sklearn.metrics import roc_auc_score\n","from transformers import RobertaModel, RobertaTokenizer\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","metadata":{},"source":["## Submission Flag"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.132784Z","iopub.status.idle":"2023-11-27T05:02:18.133100Z","shell.execute_reply":"2023-11-27T05:02:18.132967Z","shell.execute_reply.started":"2023-11-27T05:02:18.132951Z"},"trusted":true},"outputs":[],"source":["is_submission = False"]},{"cell_type":"markdown","metadata":{},"source":["## Read Datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.134286Z","iopub.status.idle":"2023-11-27T05:02:18.134626Z","shell.execute_reply":"2023-11-27T05:02:18.134478Z","shell.execute_reply.started":"2023-11-27T05:02:18.134461Z"},"trusted":true},"outputs":[],"source":["train_path1 = r\"data\\train_v2_drcat_02.csv\" if not is_submission else r\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\"\n","train_path2 = r\"data\\train_essays.csv\" if not is_submission else r\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\"\n","test_path = r\"data\\test_essays.csv\" if not is_submission else r\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\"\n","train_data1 = pd.read_csv(train_path1)\n","train_data1.rename(columns={'label': 'generated'}, inplace=True)\n","train_data2 = pd.read_csv(train_path2)\n","test_data = pd.read_csv(test_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.135608Z","iopub.status.idle":"2023-11-27T05:02:18.135935Z","shell.execute_reply":"2023-11-27T05:02:18.135779Z","shell.execute_reply.started":"2023-11-27T05:02:18.135762Z"},"trusted":true},"outputs":[{"data":{"text/plain":["generated\n","0    28746\n","1    17500\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.concat([train_data1[['text','generated']], train_data2[['text','generated']]])\n","train['text'] = train['text'].str.replace('\\n', '')\n","test_data['text'] = test_data['text'].str.replace('\\n', '')\n","train['generated'].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.136806Z","iopub.status.idle":"2023-11-27T05:02:18.137109Z","shell.execute_reply":"2023-11-27T05:02:18.136983Z","shell.execute_reply.started":"2023-11-27T05:02:18.136968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0:  17500\n","1:  17500\n"]}],"source":["rus = RandomUnderSampler(random_state=42)\n","train_text, train_label = rus.fit_resample(train['text'].to_numpy().reshape(-1,1), train['generated'].to_numpy().reshape(-1,1))\n","print('0: ', np.count_nonzero(train_label == 0))\n","print('1: ', np.count_nonzero(train_label == 1))\n","\n","data = {'text': train_text.reshape(-1), 'generated': train_label.reshape(-1)}\n","train_data = pd.DataFrame(data)\n","\n","if not is_submission:\n","    seed=202\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    mask = np.random.rand(len(train_data)) < 0.8\n","    test_data = train_data[~mask]\n","    train_data = train_data[mask]"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.137781Z","iopub.status.idle":"2023-11-27T05:02:18.138086Z","shell.execute_reply":"2023-11-27T05:02:18.137961Z","shell.execute_reply.started":"2023-11-27T05:02:18.137946Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pre-trained RoBERTa model and tokenizer\n","model = RobertaModel.from_pretrained('roberta-base')\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","# Tokenize the text in train data\n","tokenized_train_texts = tokenizer(train_data['text'].to_list(), padding=True, truncation=True, return_tensors='pt')\n","\n","# Tokenize the text in train data\n","tokenized_test_texts = tokenizer(test_data['text'].to_list(), padding=True, truncation=True, return_tensors='pt')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.139024Z","iopub.status.idle":"2023-11-27T05:02:18.139333Z","shell.execute_reply":"2023-11-27T05:02:18.139182Z","shell.execute_reply.started":"2023-11-27T05:02:18.139167Z"},"trusted":true},"outputs":[],"source":["## Create Dataloader\n","# Convert train labels to tensor\n","train_labels_tensor = torch.tensor(train_data['generated'].values)\n","\n","# Create a train TensorDataset\n","train_dataset = TensorDataset(\n","    tokenized_train_texts['input_ids'],\n","    tokenized_train_texts['attention_mask'],\n","    train_labels_tensor\n",")\n","\n","if is_submission:\n","\n","    # Create a test TensorDataset\n","    test_dataset = TensorDataset(\n","        tokenized_test_texts['input_ids'],\n","        tokenized_test_texts['attention_mask']\n","    )\n","\n","else:\n","    # Convert text labels to tensor\n","    test_labels_tensor = torch.tensor(test_data['generated'].values)\n","\n","    # Create a test TensorDataset\n","    test_dataset = TensorDataset(\n","        tokenized_test_texts['input_ids'],\n","        tokenized_test_texts['attention_mask'],\n","        test_labels_tensor\n","    )\n","\n","# Define batch size\n","batch_size = 16  # You can adjust this based on your system's memory capacity\n","\n","# Create a DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.140611Z","iopub.status.idle":"2023-11-27T05:02:18.140954Z","shell.execute_reply":"2023-11-27T05:02:18.140784Z","shell.execute_reply.started":"2023-11-27T05:02:18.140767Z"},"trusted":true},"outputs":[],"source":["def extract_features(concrete_data_loader):\n","    features_list = []\n","    labels_list = []\n","\n","    with torch.no_grad():\n","        for batch in concrete_data_loader:\n","            batch_tokenized_texts = {\n","                'input_ids': batch[0],\n","                'attention_mask': batch[1]\n","            }\n","            \n","            # Extract token embeddings for the batch\n","            batch_outputs = model(**batch_tokenized_texts)\n","            batch_embeddings = batch_outputs.last_hidden_state\n","            \n","            # Flatten the embeddings to use as features for the batch\n","            batch_features = batch_embeddings.mean(dim=1).numpy()\n","            features_list.append(batch_features)\n","            \n","            # Get labels for the batch\n","            batch_labels = batch[2].numpy()\n","            labels_list.append(batch_labels)\n","    \n","    # Concatenate features from all batches\n","    return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n","\n","def extract_features_test_submission(concrete_data_loader):\n","    features_list = []\n","\n","    with torch.no_grad():\n","        for batch in concrete_data_loader:\n","            batch_tokenized_texts = {\n","                'input_ids': batch[0],\n","                'attention_mask': batch[1]\n","            }\n","            \n","            # Extract token embeddings for the batch\n","            batch_outputs = model(**batch_tokenized_texts)\n","            batch_embeddings = batch_outputs.last_hidden_state\n","            \n","            # Flatten the embeddings to use as features for the batch\n","            batch_features = batch_embeddings.mean(dim=1).numpy()\n","            features_list.append(batch_features)\n","            \n","    \n","    # Concatenate features from all batches\n","    return np.concatenate(features_list, axis=0)\n","\n","# Extract features using DataLoader\n","if is_submission:\n","    train_features, train_labels = extract_features(train_loader)\n","    test_features = extract_features_test_submission(tokenized_test_texts)\n","    print('Len train_features: ', len(train_features), ' Len train_labels: ', len(train_labels))\n","    print('Len train_features: ', len(test_features))\n","else: \n","    train_features, train_labels = extract_features(train_loader)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Len train_features:  27983  Len train_labels:  27983\n","Len train_features:  7017  Len train_labels:  7017\n"]}],"source":["test_features, test_labels = extract_features(test_loader)\n","print('Len train_features: ', len(train_features), ' Len train_labels: ', len(train_labels))\n","print('Len train_features: ', len(test_features), ' Len train_labels: ', len(test_labels))"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'train_features' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features[\u001b[39m0\u001b[39m]\n","\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"]}],"source":["train_features[0]"]},{"cell_type":"markdown","metadata":{},"source":["## Create Graph"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T05:02:18.206951Z","iopub.status.busy":"2023-11-27T05:02:18.206694Z","iopub.status.idle":"2023-11-27T05:02:18.225832Z","shell.execute_reply":"2023-11-27T05:02:18.224556Z","shell.execute_reply.started":"2023-11-27T05:02:18.206924Z"},"trusted":true},"outputs":[],"source":["# Calculate pairwise cosine similarity between sentence embeddings\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","similarity_matrix = cosine_similarity(train_features, train_features)\n","\n","# Convert similarity matrix to a graph (adjacency matrix)\n","import networkx as nx\n","import numpy as np\n","\n","# Threshold for considering edges\n","threshold = 0.8\n","\n","adjacency_matrix = np.where(similarity_matrix > threshold, 1, 0)\n","\n","# Create a graph from the adjacency matrix\n","graph = nx.from_numpy_array(adjacency_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dgl\n","import dgl.function as fn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dgl.nn import GATConv\n","\n","# Convert the NetworkX graph to a DGL graph\n","dgl_graph = dgl.from_networkx(graph)\n","\n","class GATClassifier(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, num_classes):\n","        super(GATClassifier, self).__init__()\n","        self.gatconv1 = GATConv(in_dim, hidden_dim, num_heads=4)\n","        self.gatconv2 = GATConv(hidden_dim * 4, num_classes, num_heads=1)\n","\n","    def forward(self, g, h):\n","        h = F.elu(self.gatconv1(g, h).flatten(1))\n","        h = self.gatconv2(g, h).mean(1)\n","        return h\n","\n","# Define GAT model\n","gat_model = GATClassifier(in_dim=sentence_embeddings.shape[1], hidden_dim=64, num_classes=2)\n","\n","# Training\n","optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Assuming you have train and test data split\n","train_mask = torch.rand(len(sentences)) < 0.8\n","test_mask = ~train_mask\n","\n","def train_model(model, optimizer, criterion, train_mask, test_mask):\n","    for epoch in range(epochs):\n","        model.train()\n","        logits = model(dgl_graph, sentence_embeddings)\n","        optimizer.zero_grad()\n","        loss = criterion(logits[train_mask], labels[train_mask])\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Evaluation\n","        model.eval()\n","        with torch.no_grad():\n","            test_logits = model(dgl_graph, sentence_embeddings)\n","            test_loss = criterion(test_logits[test_mask], labels[test_mask])\n","            print(f\"Epoch {epoch + 1}/{epochs}, Test Loss: {test_loss.item()}\")\n","\n","train_model(gat_model, optimizer, criterion, train_mask, test_mask)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6888007,"sourceId":61542,"sourceType":"competition"},{"datasetId":4005256,"sourceId":6977472,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
