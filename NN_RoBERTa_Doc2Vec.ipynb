{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T05:02:18.110470Z","iopub.status.busy":"2023-11-27T05:02:18.109937Z","iopub.status.idle":"2023-11-27T05:02:18.132224Z","shell.execute_reply":"2023-11-27T05:02:18.130837Z","shell.execute_reply.started":"2023-11-27T05:02:18.110424Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Git\\kaggle\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import regex as re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gensim.models import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","from itertools import combinations\n","import numpy as np\n","from scipy.spatial.distance import euclidean\n","from transformers import RobertaTokenizer, RobertaModel\n","from imblearn.under_sampling import RandomUnderSampler\n","import random\n","from sklearn.metrics import roc_auc_score\n","import dgl\n","import dgl.nn.pytorch as dglnn\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from dgl.nn import GATConv\n","import matplotlib.pyplot as plt\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submission Flag"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.132784Z","iopub.status.idle":"2023-11-27T05:02:18.133100Z","shell.execute_reply":"2023-11-27T05:02:18.132967Z","shell.execute_reply.started":"2023-11-27T05:02:18.132951Z"},"trusted":true},"outputs":[],"source":["is_submission = False"]},{"cell_type":"markdown","metadata":{},"source":["## Read Datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.134286Z","iopub.status.idle":"2023-11-27T05:02:18.134626Z","shell.execute_reply":"2023-11-27T05:02:18.134478Z","shell.execute_reply.started":"2023-11-27T05:02:18.134461Z"},"trusted":true},"outputs":[],"source":["train_path1 = r\"data\\train_v2_drcat_02.csv\" if not is_submission else r\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\"\n","train_path2 = r\"data\\train_essays.csv\" if not is_submission else r\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\"\n","test_path = r\"data\\test_essays.csv\" if not is_submission else r\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\"\n","train_data1 = pd.read_csv(train_path1)\n","train_data1.rename(columns={'label': 'generated'}, inplace=True)\n","train_data2 = pd.read_csv(train_path2)\n","test_data = pd.read_csv(test_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.135608Z","iopub.status.idle":"2023-11-27T05:02:18.135935Z","shell.execute_reply":"2023-11-27T05:02:18.135779Z","shell.execute_reply.started":"2023-11-27T05:02:18.135762Z"},"trusted":true},"outputs":[{"data":{"text/plain":["generated\n","0    28746\n","1    17500\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.concat([train_data1[['text','generated']], train_data2[['text','generated']]])\n","train['text'] = train['text'].str.replace('\\n', '')\n","test_data['text'] = test_data['text'].str.replace('\\n', '')\n","train['generated'].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.136806Z","iopub.status.idle":"2023-11-27T05:02:18.137109Z","shell.execute_reply":"2023-11-27T05:02:18.136983Z","shell.execute_reply.started":"2023-11-27T05:02:18.136968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0:  17500\n","1:  17500\n"]}],"source":["rus = RandomUnderSampler(random_state=42)\n","train_text, train_label = rus.fit_resample(train['text'].to_numpy().reshape(-1,1), train['generated'].to_numpy().reshape(-1,1))\n","print('0: ', np.count_nonzero(train_label == 0))\n","print('1: ', np.count_nonzero(train_label == 1))\n","\n","data = {'text': train_text.reshape(-1), 'generated': train_label.reshape(-1)}\n","all_data = pd.DataFrame(data)\n","\n","if not is_submission:\n","    seed=202\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    mask = np.random.rand(len(all_data)) < 0.8\n","    test_data = all_data[~mask]\n","    train_data = all_data[mask]"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Step 1: Data Preparation\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta = RobertaModel.from_pretrained('roberta-base')\n","# Sample list of texts with labels\n","texts_list = train_data['text'].to_list()\n","\n","labels_list = train_data['generated'].to_list()  # Sample labels (0 or 1 for classification)\n","\n","# Step 2: RoBERTa Embedding\n","\n","# Tokenize and obtain embeddings for each text entry\n","roberta_embeddings_list = []\n","\n","\n","for text in texts_list:\n","    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n","    with torch.no_grad():\n","        output = roberta(**encoded_input)\n","    last_hidden_states = output.last_hidden_state\n","    # For simplicity, let's use the [CLS] token representation as the sentence embedding\n","    cls_embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n","    roberta_embeddings_list.append(cls_embedding)\n","# Step 3: Graph Construction\n","\n","# Step 1: Convert texts to TaggedDocuments\n","tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(texts_list)]\n","\n","# Step 2: Train Doc2Vec model\n","doc2vec_model = Doc2Vec(vector_size=768, window=5, min_count=1, workers=4, epochs=20)\n","doc2vec_model.build_vocab(tagged_data)\n","doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n","\n","# Step 3: Calculate similarity/distance\n","doc2vec_embeddings_list = [doc2vec_model.infer_vector(doc.words) for doc in tagged_data]"]},{"cell_type":"markdown","metadata":{},"source":["## Create Model"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T05:02:18.206951Z","iopub.status.busy":"2023-11-27T05:02:18.206694Z","iopub.status.idle":"2023-11-27T05:02:18.225832Z","shell.execute_reply":"2023-11-27T05:02:18.224556Z","shell.execute_reply.started":"2023-11-27T05:02:18.206924Z"},"trusted":true},"outputs":[],"source":["class TwoInputClassifier(nn.Module):\n","    def __init__(self, bert_input_size, doc2vec_input_size, hidden_size):\n","        super(TwoInputClassifier, self).__init__()\n","        \n","        # Define layers for BERT input\n","        self.bert_fc = nn.Linear(bert_input_size, hidden_size)\n","        \n","        # Define layers for Doc2Vec input\n","        self.doc2vec_fc = nn.Linear(doc2vec_input_size, hidden_size)\n","        \n","        # Stack 10 hidden layers\n","        hidden_layers = []\n","        hidden_layers.append(nn.Linear(hidden_size * 2, hidden_size * 2))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(hidden_size * 2, hidden_size * 2))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(hidden_size * 2, hidden_size))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(hidden_size, 600))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(600, 500))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(500, 400))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(400, 300))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(300, 200))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(200, 100))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(100, 80))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(80, 60))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        hidden_layers.append(nn.Linear(60, 40))  # Adjusted size\n","        hidden_layers.append(nn.ReLU())\n","        self.hidden_layers = nn.Sequential(*hidden_layers)\n","        \n","        # Output layer\n","        self.output = nn.Linear(40, 1)  # Adjusted size\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, bert_input, doc2vec_input):\n","        # Pass BERT input through fully connected layers and ReLU activation\n","        bert_output = F.relu(self.bert_fc(bert_input))\n","        \n","        # Pass Doc2Vec input through fully connected layers and ReLU activation\n","        doc2vec_output = F.relu(self.doc2vec_fc(doc2vec_input))\n","        \n","        # Concatenate the outputs from BERT and Doc2Vec\n","        combined_output = torch.cat((bert_output, doc2vec_output), dim=1)\n","        \n","        # Pass through 10 hidden layers\n","        combined_output = self.hidden_layers(combined_output)\n","        \n","        # Output layer with softmax activation\n","        output = self.sigmoid(self.output(combined_output), dim=1)\n","        \n","        return output\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([27983, 768])\n","torch.Size([27983, 100])\n","torch.Size([27983])\n","Epoch [1/10], Batch [10/875], Loss: 0.6953\n","Epoch [1/10], Batch [20/875], Loss: 0.5778\n","Epoch [1/10], Batch [30/875], Loss: 0.7695\n","Epoch [1/10], Batch [40/875], Loss: 0.8351\n","Epoch [1/10], Batch [50/875], Loss: 0.7851\n","Epoch [1/10], Batch [60/875], Loss: 0.7914\n","Epoch [1/10], Batch [70/875], Loss: 0.8164\n","Epoch [1/10], Batch [80/875], Loss: 0.7851\n","Epoch [1/10], Batch [90/875], Loss: 0.8101\n","Epoch [1/10], Batch [100/875], Loss: 0.7976\n","Epoch [1/10], Batch [110/875], Loss: 0.8289\n","Epoch [1/10], Batch [120/875], Loss: 0.8508\n","Epoch [1/10], Batch [130/875], Loss: 0.7789\n","Epoch [1/10], Batch [140/875], Loss: 0.8008\n","Epoch [1/10], Batch [150/875], Loss: 0.8383\n","Epoch [1/10], Batch [160/875], Loss: 0.8039\n","Epoch [1/10], Batch [170/875], Loss: 0.8726\n","Epoch [1/10], Batch [180/875], Loss: 0.7945\n","Epoch [1/10], Batch [190/875], Loss: 0.8195\n","Epoch [1/10], Batch [200/875], Loss: 0.8445\n","Epoch [1/10], Batch [210/875], Loss: 0.8976\n","Epoch [1/10], Batch [220/875], Loss: 0.8820\n","Epoch [1/10], Batch [230/875], Loss: 0.8601\n","Epoch [1/10], Batch [240/875], Loss: 0.7914\n","Epoch [1/10], Batch [250/875], Loss: 0.7695\n","Epoch [1/10], Batch [260/875], Loss: 0.7976\n","Epoch [1/10], Batch [270/875], Loss: 0.8258\n","Epoch [1/10], Batch [280/875], Loss: 0.7914\n","Epoch [1/10], Batch [290/875], Loss: 0.7914\n","Epoch [1/10], Batch [300/875], Loss: 0.7758\n","Epoch [1/10], Batch [310/875], Loss: 0.8008\n","Epoch [1/10], Batch [320/875], Loss: 0.8226\n","Epoch [1/10], Batch [330/875], Loss: 0.7914\n","Epoch [1/10], Batch [340/875], Loss: 0.8164\n","Epoch [1/10], Batch [350/875], Loss: 0.8226\n","Epoch [1/10], Batch [360/875], Loss: 0.8164\n","Epoch [1/10], Batch [370/875], Loss: 0.7914\n","Epoch [1/10], Batch [380/875], Loss: 0.8383\n","Epoch [1/10], Batch [390/875], Loss: 0.8101\n","Epoch [1/10], Batch [400/875], Loss: 0.8133\n","Epoch [1/10], Batch [410/875], Loss: 0.8258\n","Epoch [1/10], Batch [420/875], Loss: 0.8195\n","Epoch [1/10], Batch [430/875], Loss: 0.7976\n","Epoch [1/10], Batch [440/875], Loss: 0.8320\n","Epoch [1/10], Batch [450/875], Loss: 0.8258\n","Epoch [1/10], Batch [460/875], Loss: 0.9039\n","Epoch [1/10], Batch [470/875], Loss: 0.8445\n","Epoch [1/10], Batch [480/875], Loss: 0.8289\n","Epoch [1/10], Batch [490/875], Loss: 0.7883\n","Epoch [1/10], Batch [500/875], Loss: 0.7695\n","Epoch [1/10], Batch [510/875], Loss: 0.8445\n","Epoch [1/10], Batch [520/875], Loss: 0.7976\n","Epoch [1/10], Batch [530/875], Loss: 0.7633\n","Epoch [1/10], Batch [540/875], Loss: 0.8539\n","Epoch [1/10], Batch [550/875], Loss: 0.7570\n","Epoch [1/10], Batch [560/875], Loss: 0.7851\n","Epoch [1/10], Batch [570/875], Loss: 0.7726\n","Epoch [1/10], Batch [580/875], Loss: 0.7976\n","Epoch [1/10], Batch [590/875], Loss: 0.8008\n","Epoch [1/10], Batch [600/875], Loss: 0.7726\n","Epoch [1/10], Batch [610/875], Loss: 0.8351\n","Epoch [1/10], Batch [620/875], Loss: 0.8226\n","Epoch [1/10], Batch [630/875], Loss: 0.8039\n","Epoch [1/10], Batch [640/875], Loss: 0.8133\n","Epoch [1/10], Batch [650/875], Loss: 0.8258\n","Epoch [1/10], Batch [660/875], Loss: 0.8133\n","Epoch [1/10], Batch [670/875], Loss: 0.8101\n","Epoch [1/10], Batch [680/875], Loss: 0.7601\n","Epoch [1/10], Batch [690/875], Loss: 0.8320\n","Epoch [1/10], Batch [700/875], Loss: 0.7851\n","Epoch [1/10], Batch [710/875], Loss: 0.7976\n","Epoch [1/10], Batch [720/875], Loss: 0.7976\n","Epoch [1/10], Batch [730/875], Loss: 0.8445\n","Epoch [1/10], Batch [740/875], Loss: 0.8008\n","Epoch [1/10], Batch [750/875], Loss: 0.8383\n","Epoch [1/10], Batch [760/875], Loss: 0.8008\n","Epoch [1/10], Batch [770/875], Loss: 0.8258\n","Epoch [1/10], Batch [780/875], Loss: 0.8133\n","Epoch [1/10], Batch [790/875], Loss: 0.8601\n","Epoch [1/10], Batch [800/875], Loss: 0.8226\n","Epoch [1/10], Batch [810/875], Loss: 0.8195\n","Epoch [1/10], Batch [820/875], Loss: 0.8070\n","Epoch [1/10], Batch [830/875], Loss: 0.7851\n","Epoch [1/10], Batch [840/875], Loss: 0.8258\n","Epoch [1/10], Batch [850/875], Loss: 0.8258\n","Epoch [1/10], Batch [860/875], Loss: 0.8164\n","Epoch [1/10], Batch [870/875], Loss: 0.8476\n","Epoch [2/10], Batch [10/875], Loss: 0.8508\n","Epoch [2/10], Batch [20/875], Loss: 0.8133\n","Epoch [2/10], Batch [30/875], Loss: 0.8414\n","Epoch [2/10], Batch [40/875], Loss: 0.8258\n","Epoch [2/10], Batch [50/875], Loss: 0.8289\n","Epoch [2/10], Batch [60/875], Loss: 0.7883\n","Epoch [2/10], Batch [70/875], Loss: 0.8008\n","Epoch [2/10], Batch [80/875], Loss: 0.8383\n","Epoch [2/10], Batch [90/875], Loss: 0.7789\n","Epoch [2/10], Batch [100/875], Loss: 0.7851\n","Epoch [2/10], Batch [110/875], Loss: 0.8539\n","Epoch [2/10], Batch [120/875], Loss: 0.8289\n","Epoch [2/10], Batch [130/875], Loss: 0.8289\n","Epoch [2/10], Batch [140/875], Loss: 0.7820\n","Epoch [2/10], Batch [150/875], Loss: 0.8414\n","Epoch [2/10], Batch [160/875], Loss: 0.8320\n","Epoch [2/10], Batch [170/875], Loss: 0.7945\n","Epoch [2/10], Batch [180/875], Loss: 0.8758\n","Epoch [2/10], Batch [190/875], Loss: 0.8101\n","Epoch [2/10], Batch [200/875], Loss: 0.8383\n","Epoch [2/10], Batch [210/875], Loss: 0.8070\n","Epoch [2/10], Batch [220/875], Loss: 0.8195\n","Epoch [2/10], Batch [230/875], Loss: 0.8258\n","Epoch [2/10], Batch [240/875], Loss: 0.8008\n","Epoch [2/10], Batch [250/875], Loss: 0.8539\n","Epoch [2/10], Batch [260/875], Loss: 0.7789\n","Epoch [2/10], Batch [270/875], Loss: 0.7914\n","Epoch [2/10], Batch [280/875], Loss: 0.8320\n","Epoch [2/10], Batch [290/875], Loss: 0.8226\n","Epoch [2/10], Batch [300/875], Loss: 0.8351\n","Epoch [2/10], Batch [310/875], Loss: 0.7914\n","Epoch [2/10], Batch [320/875], Loss: 0.8133\n","Epoch [2/10], Batch [330/875], Loss: 0.8539\n","Epoch [2/10], Batch [340/875], Loss: 0.8226\n","Epoch [2/10], Batch [350/875], Loss: 0.8070\n","Epoch [2/10], Batch [360/875], Loss: 0.7851\n","Epoch [2/10], Batch [370/875], Loss: 0.8508\n","Epoch [2/10], Batch [380/875], Loss: 0.8008\n","Epoch [2/10], Batch [390/875], Loss: 0.7445\n","Epoch [2/10], Batch [400/875], Loss: 0.7758\n","Epoch [2/10], Batch [410/875], Loss: 0.7726\n","Epoch [2/10], Batch [420/875], Loss: 0.8351\n","Epoch [2/10], Batch [430/875], Loss: 0.8039\n","Epoch [2/10], Batch [440/875], Loss: 0.7945\n","Epoch [2/10], Batch [450/875], Loss: 0.8070\n","Epoch [2/10], Batch [460/875], Loss: 0.7851\n","Epoch [2/10], Batch [470/875], Loss: 0.8476\n","Epoch [2/10], Batch [480/875], Loss: 0.7914\n","Epoch [2/10], Batch [490/875], Loss: 0.8101\n","Epoch [2/10], Batch [500/875], Loss: 0.7601\n","Epoch [2/10], Batch [510/875], Loss: 0.8351\n","Epoch [2/10], Batch [520/875], Loss: 0.7945\n","Epoch [2/10], Batch [530/875], Loss: 0.7820\n","Epoch [2/10], Batch [540/875], Loss: 0.8351\n","Epoch [2/10], Batch [550/875], Loss: 0.8164\n","Epoch [2/10], Batch [560/875], Loss: 0.8414\n","Epoch [2/10], Batch [570/875], Loss: 0.8758\n","Epoch [2/10], Batch [580/875], Loss: 0.8039\n","Epoch [2/10], Batch [590/875], Loss: 0.8101\n","Epoch [2/10], Batch [600/875], Loss: 0.7945\n","Epoch [2/10], Batch [610/875], Loss: 0.7883\n","Epoch [2/10], Batch [620/875], Loss: 0.8258\n","Epoch [2/10], Batch [630/875], Loss: 0.7945\n","Epoch [2/10], Batch [640/875], Loss: 0.7758\n","Epoch [2/10], Batch [650/875], Loss: 0.7883\n","Epoch [2/10], Batch [660/875], Loss: 0.8039\n","Epoch [2/10], Batch [670/875], Loss: 0.8789\n","Epoch [2/10], Batch [680/875], Loss: 0.7914\n","Epoch [2/10], Batch [690/875], Loss: 0.8226\n","Epoch [2/10], Batch [700/875], Loss: 0.8195\n","Epoch [2/10], Batch [710/875], Loss: 0.8383\n","Epoch [2/10], Batch [720/875], Loss: 0.7851\n","Epoch [2/10], Batch [730/875], Loss: 0.8570\n","Epoch [2/10], Batch [740/875], Loss: 0.8133\n","Epoch [2/10], Batch [750/875], Loss: 0.8351\n","Epoch [2/10], Batch [760/875], Loss: 0.8414\n","Epoch [2/10], Batch [770/875], Loss: 0.7914\n","Epoch [2/10], Batch [780/875], Loss: 0.7664\n","Epoch [2/10], Batch [790/875], Loss: 0.8476\n","Epoch [2/10], Batch [800/875], Loss: 0.7726\n","Epoch [2/10], Batch [810/875], Loss: 0.8289\n","Epoch [2/10], Batch [820/875], Loss: 0.8351\n","Epoch [2/10], Batch [830/875], Loss: 0.8351\n","Epoch [2/10], Batch [840/875], Loss: 0.7945\n","Epoch [2/10], Batch [850/875], Loss: 0.8070\n","Epoch [2/10], Batch [860/875], Loss: 0.8508\n","Epoch [2/10], Batch [870/875], Loss: 0.7851\n","Epoch [3/10], Batch [10/875], Loss: 0.8601\n","Epoch [3/10], Batch [20/875], Loss: 0.8195\n","Epoch [3/10], Batch [30/875], Loss: 0.8383\n","Epoch [3/10], Batch [40/875], Loss: 0.8289\n","Epoch [3/10], Batch [50/875], Loss: 0.7976\n","Epoch [3/10], Batch [60/875], Loss: 0.7758\n","Epoch [3/10], Batch [70/875], Loss: 0.8383\n","Epoch [3/10], Batch [80/875], Loss: 0.8258\n","Epoch [3/10], Batch [90/875], Loss: 0.8070\n","Epoch [3/10], Batch [100/875], Loss: 0.8383\n","Epoch [3/10], Batch [110/875], Loss: 0.8351\n","Epoch [3/10], Batch [120/875], Loss: 0.8164\n","Epoch [3/10], Batch [130/875], Loss: 0.8070\n","Epoch [3/10], Batch [140/875], Loss: 0.8226\n","Epoch [3/10], Batch [150/875], Loss: 0.8320\n","Epoch [3/10], Batch [160/875], Loss: 0.8101\n","Epoch [3/10], Batch [170/875], Loss: 0.8039\n","Epoch [3/10], Batch [180/875], Loss: 0.7851\n","Epoch [3/10], Batch [190/875], Loss: 0.7945\n","Epoch [3/10], Batch [200/875], Loss: 0.8164\n","Epoch [3/10], Batch [210/875], Loss: 0.8195\n","Epoch [3/10], Batch [220/875], Loss: 0.7789\n","Epoch [3/10], Batch [230/875], Loss: 0.8008\n","Epoch [3/10], Batch [240/875], Loss: 0.8226\n","Epoch [3/10], Batch [250/875], Loss: 0.7695\n","Epoch [3/10], Batch [260/875], Loss: 0.8445\n","Epoch [3/10], Batch [270/875], Loss: 0.8414\n","Epoch [3/10], Batch [280/875], Loss: 0.8476\n","Epoch [3/10], Batch [290/875], Loss: 0.7789\n","Epoch [3/10], Batch [300/875], Loss: 0.8133\n","Epoch [3/10], Batch [310/875], Loss: 0.8226\n","Epoch [3/10], Batch [320/875], Loss: 0.7820\n","Epoch [3/10], Batch [330/875], Loss: 0.7820\n","Epoch [3/10], Batch [340/875], Loss: 0.8414\n","Epoch [3/10], Batch [350/875], Loss: 0.8164\n","Epoch [3/10], Batch [360/875], Loss: 0.8133\n","Epoch [3/10], Batch [370/875], Loss: 0.8258\n","Epoch [3/10], Batch [380/875], Loss: 0.8164\n","Epoch [3/10], Batch [390/875], Loss: 0.7945\n","Epoch [3/10], Batch [400/875], Loss: 0.8476\n","Epoch [3/10], Batch [410/875], Loss: 0.8289\n","Epoch [3/10], Batch [420/875], Loss: 0.8133\n","Epoch [3/10], Batch [430/875], Loss: 0.8758\n","Epoch [3/10], Batch [440/875], Loss: 0.8164\n","Epoch [3/10], Batch [450/875], Loss: 0.8039\n","Epoch [3/10], Batch [460/875], Loss: 0.8008\n","Epoch [3/10], Batch [470/875], Loss: 0.7976\n","Epoch [3/10], Batch [480/875], Loss: 0.8039\n","Epoch [3/10], Batch [490/875], Loss: 0.8008\n","Epoch [3/10], Batch [500/875], Loss: 0.7851\n","Epoch [3/10], Batch [510/875], Loss: 0.8133\n","Epoch [3/10], Batch [520/875], Loss: 0.8695\n","Epoch [3/10], Batch [530/875], Loss: 0.7789\n","Epoch [3/10], Batch [540/875], Loss: 0.8008\n","Epoch [3/10], Batch [550/875], Loss: 0.8133\n","Epoch [3/10], Batch [560/875], Loss: 0.8445\n","Epoch [3/10], Batch [570/875], Loss: 0.7976\n","Epoch [3/10], Batch [580/875], Loss: 0.8351\n","Epoch [3/10], Batch [590/875], Loss: 0.7851\n","Epoch [3/10], Batch [600/875], Loss: 0.7758\n","Epoch [3/10], Batch [610/875], Loss: 0.8289\n","Epoch [3/10], Batch [620/875], Loss: 0.7883\n","Epoch [3/10], Batch [630/875], Loss: 0.8289\n","Epoch [3/10], Batch [640/875], Loss: 0.7945\n","Epoch [3/10], Batch [650/875], Loss: 0.7664\n","Epoch [3/10], Batch [660/875], Loss: 0.7851\n","Epoch [3/10], Batch [670/875], Loss: 0.8195\n","Epoch [3/10], Batch [680/875], Loss: 0.8320\n","Epoch [3/10], Batch [690/875], Loss: 0.7695\n","Epoch [3/10], Batch [700/875], Loss: 0.8664\n","Epoch [3/10], Batch [710/875], Loss: 0.8383\n","Epoch [3/10], Batch [720/875], Loss: 0.8383\n","Epoch [3/10], Batch [730/875], Loss: 0.7976\n","Epoch [3/10], Batch [740/875], Loss: 0.8383\n","Epoch [3/10], Batch [750/875], Loss: 0.8164\n","Epoch [3/10], Batch [760/875], Loss: 0.7883\n","Epoch [3/10], Batch [770/875], Loss: 0.7914\n","Epoch [3/10], Batch [780/875], Loss: 0.8383\n","Epoch [3/10], Batch [790/875], Loss: 0.7820\n","Epoch [3/10], Batch [800/875], Loss: 0.8570\n","Epoch [3/10], Batch [810/875], Loss: 0.7883\n","Epoch [3/10], Batch [820/875], Loss: 0.8133\n","Epoch [3/10], Batch [830/875], Loss: 0.7851\n","Epoch [3/10], Batch [840/875], Loss: 0.8195\n","Epoch [3/10], Batch [850/875], Loss: 0.8570\n","Epoch [3/10], Batch [860/875], Loss: 0.8226\n","Epoch [3/10], Batch [870/875], Loss: 0.8070\n","Epoch [4/10], Batch [10/875], Loss: 0.7726\n","Epoch [4/10], Batch [20/875], Loss: 0.8289\n","Epoch [4/10], Batch [30/875], Loss: 0.8320\n","Epoch [4/10], Batch [40/875], Loss: 0.8258\n","Epoch [4/10], Batch [50/875], Loss: 0.8383\n","Epoch [4/10], Batch [60/875], Loss: 0.8539\n","Epoch [4/10], Batch [70/875], Loss: 0.7789\n","Epoch [4/10], Batch [80/875], Loss: 0.8320\n","Epoch [4/10], Batch [90/875], Loss: 0.8570\n","Epoch [4/10], Batch [100/875], Loss: 0.8039\n","Epoch [4/10], Batch [110/875], Loss: 0.7851\n","Epoch [4/10], Batch [120/875], Loss: 0.8164\n","Epoch [4/10], Batch [130/875], Loss: 0.7851\n","Epoch [4/10], Batch [140/875], Loss: 0.8226\n","Epoch [4/10], Batch [150/875], Loss: 0.7539\n","Epoch [4/10], Batch [160/875], Loss: 0.8289\n","Epoch [4/10], Batch [170/875], Loss: 0.7445\n","Epoch [4/10], Batch [180/875], Loss: 0.8289\n","Epoch [4/10], Batch [190/875], Loss: 0.8570\n","Epoch [4/10], Batch [200/875], Loss: 0.8414\n","Epoch [4/10], Batch [210/875], Loss: 0.8164\n","Epoch [4/10], Batch [220/875], Loss: 0.7570\n","Epoch [4/10], Batch [230/875], Loss: 0.8664\n","Epoch [4/10], Batch [240/875], Loss: 0.8414\n","Epoch [4/10], Batch [250/875], Loss: 0.8226\n","Epoch [4/10], Batch [260/875], Loss: 0.8226\n","Epoch [4/10], Batch [270/875], Loss: 0.8164\n","Epoch [4/10], Batch [280/875], Loss: 0.8289\n","Epoch [4/10], Batch [290/875], Loss: 0.8226\n","Epoch [4/10], Batch [300/875], Loss: 0.8508\n","Epoch [4/10], Batch [310/875], Loss: 0.8133\n","Epoch [4/10], Batch [320/875], Loss: 0.7664\n","Epoch [4/10], Batch [330/875], Loss: 0.7570\n","Epoch [4/10], Batch [340/875], Loss: 0.8289\n","Epoch [4/10], Batch [350/875], Loss: 0.8320\n","Epoch [4/10], Batch [360/875], Loss: 0.8258\n","Epoch [4/10], Batch [370/875], Loss: 0.7914\n","Epoch [4/10], Batch [380/875], Loss: 0.8039\n","Epoch [4/10], Batch [390/875], Loss: 0.8070\n","Epoch [4/10], Batch [400/875], Loss: 0.8476\n","Epoch [4/10], Batch [410/875], Loss: 0.8039\n","Epoch [4/10], Batch [420/875], Loss: 0.8320\n","Epoch [4/10], Batch [430/875], Loss: 0.7976\n","Epoch [4/10], Batch [440/875], Loss: 0.8320\n","Epoch [4/10], Batch [450/875], Loss: 0.7570\n","Epoch [4/10], Batch [460/875], Loss: 0.8101\n","Epoch [4/10], Batch [470/875], Loss: 0.8320\n","Epoch [4/10], Batch [480/875], Loss: 0.8226\n","Epoch [4/10], Batch [490/875], Loss: 0.8101\n","Epoch [4/10], Batch [500/875], Loss: 0.8320\n","Epoch [4/10], Batch [510/875], Loss: 0.7539\n","Epoch [4/10], Batch [520/875], Loss: 0.8539\n","Epoch [4/10], Batch [530/875], Loss: 0.8070\n","Epoch [4/10], Batch [540/875], Loss: 0.8164\n","Epoch [4/10], Batch [550/875], Loss: 0.8695\n","Epoch [4/10], Batch [560/875], Loss: 0.7914\n","Epoch [4/10], Batch [570/875], Loss: 0.8195\n","Epoch [4/10], Batch [580/875], Loss: 0.8008\n","Epoch [4/10], Batch [590/875], Loss: 0.8008\n","Epoch [4/10], Batch [600/875], Loss: 0.8101\n","Epoch [4/10], Batch [610/875], Loss: 0.8226\n","Epoch [4/10], Batch [620/875], Loss: 0.8101\n","Epoch [4/10], Batch [630/875], Loss: 0.7945\n","Epoch [4/10], Batch [640/875], Loss: 0.8070\n","Epoch [4/10], Batch [650/875], Loss: 0.7914\n","Epoch [4/10], Batch [660/875], Loss: 0.8164\n","Epoch [4/10], Batch [670/875], Loss: 0.7976\n","Epoch [4/10], Batch [680/875], Loss: 0.8351\n","Epoch [4/10], Batch [690/875], Loss: 0.8133\n","Epoch [4/10], Batch [700/875], Loss: 0.7726\n","Epoch [4/10], Batch [710/875], Loss: 0.8008\n","Epoch [4/10], Batch [720/875], Loss: 0.7945\n","Epoch [4/10], Batch [730/875], Loss: 0.8508\n","Epoch [4/10], Batch [740/875], Loss: 0.7883\n","Epoch [4/10], Batch [750/875], Loss: 0.8289\n","Epoch [4/10], Batch [760/875], Loss: 0.8164\n","Epoch [4/10], Batch [770/875], Loss: 0.8101\n","Epoch [4/10], Batch [780/875], Loss: 0.7570\n","Epoch [4/10], Batch [790/875], Loss: 0.7976\n","Epoch [4/10], Batch [800/875], Loss: 0.8133\n","Epoch [4/10], Batch [810/875], Loss: 0.8414\n","Epoch [4/10], Batch [820/875], Loss: 0.7820\n","Epoch [4/10], Batch [830/875], Loss: 0.8351\n","Epoch [4/10], Batch [840/875], Loss: 0.8445\n","Epoch [4/10], Batch [850/875], Loss: 0.8226\n","Epoch [4/10], Batch [860/875], Loss: 0.8414\n","Epoch [4/10], Batch [870/875], Loss: 0.8601\n","Epoch [5/10], Batch [10/875], Loss: 0.8320\n","Epoch [5/10], Batch [20/875], Loss: 0.8195\n","Epoch [5/10], Batch [30/875], Loss: 0.7945\n","Epoch [5/10], Batch [40/875], Loss: 0.8445\n","Epoch [5/10], Batch [50/875], Loss: 0.8320\n","Epoch [5/10], Batch [60/875], Loss: 0.8133\n","Epoch [5/10], Batch [70/875], Loss: 0.8601\n","Epoch [5/10], Batch [80/875], Loss: 0.7539\n","Epoch [5/10], Batch [90/875], Loss: 0.8133\n","Epoch [5/10], Batch [100/875], Loss: 0.7820\n","Epoch [5/10], Batch [110/875], Loss: 0.8195\n","Epoch [5/10], Batch [120/875], Loss: 0.8570\n","Epoch [5/10], Batch [130/875], Loss: 0.7851\n","Epoch [5/10], Batch [140/875], Loss: 0.8133\n","Epoch [5/10], Batch [150/875], Loss: 0.8195\n","Epoch [5/10], Batch [160/875], Loss: 0.7414\n","Epoch [5/10], Batch [170/875], Loss: 0.7883\n","Epoch [5/10], Batch [180/875], Loss: 0.8008\n","Epoch [5/10], Batch [190/875], Loss: 0.7914\n","Epoch [5/10], Batch [200/875], Loss: 0.8164\n","Epoch [5/10], Batch [210/875], Loss: 0.8539\n","Epoch [5/10], Batch [220/875], Loss: 0.8445\n","Epoch [5/10], Batch [230/875], Loss: 0.8570\n","Epoch [5/10], Batch [240/875], Loss: 0.8008\n","Epoch [5/10], Batch [250/875], Loss: 0.8195\n","Epoch [5/10], Batch [260/875], Loss: 0.7883\n","Epoch [5/10], Batch [270/875], Loss: 0.8383\n","Epoch [5/10], Batch [280/875], Loss: 0.8133\n","Epoch [5/10], Batch [290/875], Loss: 0.7820\n","Epoch [5/10], Batch [300/875], Loss: 0.7976\n","Epoch [5/10], Batch [310/875], Loss: 0.8320\n","Epoch [5/10], Batch [320/875], Loss: 0.7820\n","Epoch [5/10], Batch [330/875], Loss: 0.8289\n","Epoch [5/10], Batch [340/875], Loss: 0.7851\n","Epoch [5/10], Batch [350/875], Loss: 0.7883\n","Epoch [5/10], Batch [360/875], Loss: 0.8101\n","Epoch [5/10], Batch [370/875], Loss: 0.8508\n","Epoch [5/10], Batch [380/875], Loss: 0.8383\n","Epoch [5/10], Batch [390/875], Loss: 0.8133\n","Epoch [5/10], Batch [400/875], Loss: 0.8226\n","Epoch [5/10], Batch [410/875], Loss: 0.8289\n","Epoch [5/10], Batch [420/875], Loss: 0.8445\n","Epoch [5/10], Batch [430/875], Loss: 0.7789\n","Epoch [5/10], Batch [440/875], Loss: 0.8008\n","Epoch [5/10], Batch [450/875], Loss: 0.8351\n","Epoch [5/10], Batch [460/875], Loss: 0.8320\n","Epoch [5/10], Batch [470/875], Loss: 0.8008\n","Epoch [5/10], Batch [480/875], Loss: 0.8164\n","Epoch [5/10], Batch [490/875], Loss: 0.8414\n","Epoch [5/10], Batch [500/875], Loss: 0.8039\n","Epoch [5/10], Batch [510/875], Loss: 0.8445\n","Epoch [5/10], Batch [520/875], Loss: 0.7508\n","Epoch [5/10], Batch [530/875], Loss: 0.7570\n","Epoch [5/10], Batch [540/875], Loss: 0.8695\n","Epoch [5/10], Batch [550/875], Loss: 0.7820\n","Epoch [5/10], Batch [560/875], Loss: 0.8195\n","Epoch [5/10], Batch [570/875], Loss: 0.8008\n","Epoch [5/10], Batch [580/875], Loss: 0.8445\n","Epoch [5/10], Batch [590/875], Loss: 0.8101\n","Epoch [5/10], Batch [600/875], Loss: 0.8195\n","Epoch [5/10], Batch [610/875], Loss: 0.7695\n","Epoch [5/10], Batch [620/875], Loss: 0.8476\n","Epoch [5/10], Batch [630/875], Loss: 0.7851\n","Epoch [5/10], Batch [640/875], Loss: 0.8226\n","Epoch [5/10], Batch [650/875], Loss: 0.8508\n","Epoch [5/10], Batch [660/875], Loss: 0.8164\n","Epoch [5/10], Batch [670/875], Loss: 0.7758\n","Epoch [5/10], Batch [680/875], Loss: 0.8101\n","Epoch [5/10], Batch [690/875], Loss: 0.7883\n","Epoch [5/10], Batch [700/875], Loss: 0.7726\n","Epoch [5/10], Batch [710/875], Loss: 0.8164\n","Epoch [5/10], Batch [720/875], Loss: 0.8539\n","Epoch [5/10], Batch [730/875], Loss: 0.8664\n","Epoch [5/10], Batch [740/875], Loss: 0.7789\n","Epoch [5/10], Batch [750/875], Loss: 0.7914\n","Epoch [5/10], Batch [760/875], Loss: 0.8351\n","Epoch [5/10], Batch [770/875], Loss: 0.8164\n","Epoch [5/10], Batch [780/875], Loss: 0.8445\n","Epoch [5/10], Batch [790/875], Loss: 0.8258\n","Epoch [5/10], Batch [800/875], Loss: 0.8445\n","Epoch [5/10], Batch [810/875], Loss: 0.8476\n","Epoch [5/10], Batch [820/875], Loss: 0.8445\n","Epoch [5/10], Batch [830/875], Loss: 0.7976\n","Epoch [5/10], Batch [840/875], Loss: 0.8195\n","Epoch [5/10], Batch [850/875], Loss: 0.8164\n","Epoch [5/10], Batch [860/875], Loss: 0.8101\n","Epoch [5/10], Batch [870/875], Loss: 0.7695\n","Epoch [6/10], Batch [10/875], Loss: 0.7851\n","Epoch [6/10], Batch [20/875], Loss: 0.7851\n","Epoch [6/10], Batch [30/875], Loss: 0.7976\n","Epoch [6/10], Batch [40/875], Loss: 0.7883\n","Epoch [6/10], Batch [50/875], Loss: 0.7820\n","Epoch [6/10], Batch [60/875], Loss: 0.8195\n","Epoch [6/10], Batch [70/875], Loss: 0.8008\n","Epoch [6/10], Batch [80/875], Loss: 0.8008\n","Epoch [6/10], Batch [90/875], Loss: 0.8570\n","Epoch [6/10], Batch [100/875], Loss: 0.7726\n","Epoch [6/10], Batch [110/875], Loss: 0.8976\n","Epoch [6/10], Batch [120/875], Loss: 0.8101\n","Epoch [6/10], Batch [130/875], Loss: 0.8195\n","Epoch [6/10], Batch [140/875], Loss: 0.8070\n","Epoch [6/10], Batch [150/875], Loss: 0.8226\n","Epoch [6/10], Batch [160/875], Loss: 0.8164\n","Epoch [6/10], Batch [170/875], Loss: 0.8039\n","Epoch [6/10], Batch [180/875], Loss: 0.8226\n","Epoch [6/10], Batch [190/875], Loss: 0.8039\n","Epoch [6/10], Batch [200/875], Loss: 0.7883\n","Epoch [6/10], Batch [210/875], Loss: 0.8351\n","Epoch [6/10], Batch [220/875], Loss: 0.8445\n","Epoch [6/10], Batch [230/875], Loss: 0.8258\n","Epoch [6/10], Batch [240/875], Loss: 0.8351\n","Epoch [6/10], Batch [250/875], Loss: 0.8133\n","Epoch [6/10], Batch [260/875], Loss: 0.8383\n","Epoch [6/10], Batch [270/875], Loss: 0.8195\n","Epoch [6/10], Batch [280/875], Loss: 0.7883\n","Epoch [6/10], Batch [290/875], Loss: 0.8320\n","Epoch [6/10], Batch [300/875], Loss: 0.7664\n","Epoch [6/10], Batch [310/875], Loss: 0.7758\n","Epoch [6/10], Batch [320/875], Loss: 0.7976\n","Epoch [6/10], Batch [330/875], Loss: 0.8226\n","Epoch [6/10], Batch [340/875], Loss: 0.7226\n","Epoch [6/10], Batch [350/875], Loss: 0.7976\n","Epoch [6/10], Batch [360/875], Loss: 0.8320\n","Epoch [6/10], Batch [370/875], Loss: 0.7914\n","Epoch [6/10], Batch [380/875], Loss: 0.8164\n","Epoch [6/10], Batch [390/875], Loss: 0.7851\n","Epoch [6/10], Batch [400/875], Loss: 0.8414\n","Epoch [6/10], Batch [410/875], Loss: 0.7789\n","Epoch [6/10], Batch [420/875], Loss: 0.8726\n","Epoch [6/10], Batch [430/875], Loss: 0.8258\n","Epoch [6/10], Batch [440/875], Loss: 0.8070\n","Epoch [6/10], Batch [450/875], Loss: 0.7789\n","Epoch [6/10], Batch [460/875], Loss: 0.8508\n","Epoch [6/10], Batch [470/875], Loss: 0.8414\n","Epoch [6/10], Batch [480/875], Loss: 0.8226\n","Epoch [6/10], Batch [490/875], Loss: 0.8414\n","Epoch [6/10], Batch [500/875], Loss: 0.8164\n","Epoch [6/10], Batch [510/875], Loss: 0.8039\n","Epoch [6/10], Batch [520/875], Loss: 0.8445\n","Epoch [6/10], Batch [530/875], Loss: 0.8414\n","Epoch [6/10], Batch [540/875], Loss: 0.8008\n","Epoch [6/10], Batch [550/875], Loss: 0.8070\n","Epoch [6/10], Batch [560/875], Loss: 0.8101\n","Epoch [6/10], Batch [570/875], Loss: 0.8445\n","Epoch [6/10], Batch [580/875], Loss: 0.8539\n","Epoch [6/10], Batch [590/875], Loss: 0.8133\n","Epoch [6/10], Batch [600/875], Loss: 0.7633\n","Epoch [6/10], Batch [610/875], Loss: 0.8226\n","Epoch [6/10], Batch [620/875], Loss: 0.7695\n","Epoch [6/10], Batch [630/875], Loss: 0.8164\n","Epoch [6/10], Batch [640/875], Loss: 0.7883\n","Epoch [6/10], Batch [650/875], Loss: 0.8133\n","Epoch [6/10], Batch [660/875], Loss: 0.8258\n","Epoch [6/10], Batch [670/875], Loss: 0.8539\n","Epoch [6/10], Batch [680/875], Loss: 0.8601\n","Epoch [6/10], Batch [690/875], Loss: 0.8133\n","Epoch [6/10], Batch [700/875], Loss: 0.7945\n","Epoch [6/10], Batch [710/875], Loss: 0.8258\n","Epoch [6/10], Batch [720/875], Loss: 0.8226\n","Epoch [6/10], Batch [730/875], Loss: 0.8070\n","Epoch [6/10], Batch [740/875], Loss: 0.8476\n","Epoch [6/10], Batch [750/875], Loss: 0.8039\n","Epoch [6/10], Batch [760/875], Loss: 0.8445\n","Epoch [6/10], Batch [770/875], Loss: 0.8039\n","Epoch [6/10], Batch [780/875], Loss: 0.8414\n","Epoch [6/10], Batch [790/875], Loss: 0.8101\n","Epoch [6/10], Batch [800/875], Loss: 0.8070\n","Epoch [6/10], Batch [810/875], Loss: 0.7820\n","Epoch [6/10], Batch [820/875], Loss: 0.7851\n","Epoch [6/10], Batch [830/875], Loss: 0.8039\n","Epoch [6/10], Batch [840/875], Loss: 0.8664\n","Epoch [6/10], Batch [850/875], Loss: 0.8133\n","Epoch [6/10], Batch [860/875], Loss: 0.8008\n","Epoch [6/10], Batch [870/875], Loss: 0.8226\n","Epoch [7/10], Batch [10/875], Loss: 0.8101\n","Epoch [7/10], Batch [20/875], Loss: 0.8508\n","Epoch [7/10], Batch [30/875], Loss: 0.8164\n","Epoch [7/10], Batch [40/875], Loss: 0.7758\n","Epoch [7/10], Batch [50/875], Loss: 0.7976\n","Epoch [7/10], Batch [60/875], Loss: 0.8101\n","Epoch [7/10], Batch [70/875], Loss: 0.7883\n","Epoch [7/10], Batch [80/875], Loss: 0.8226\n","Epoch [7/10], Batch [90/875], Loss: 0.7476\n","Epoch [7/10], Batch [100/875], Loss: 0.8226\n","Epoch [7/10], Batch [110/875], Loss: 0.8008\n","Epoch [7/10], Batch [120/875], Loss: 0.8070\n","Epoch [7/10], Batch [130/875], Loss: 0.8476\n","Epoch [7/10], Batch [140/875], Loss: 0.8164\n","Epoch [7/10], Batch [150/875], Loss: 0.8383\n","Epoch [7/10], Batch [160/875], Loss: 0.8258\n","Epoch [7/10], Batch [170/875], Loss: 0.8101\n","Epoch [7/10], Batch [180/875], Loss: 0.7945\n","Epoch [7/10], Batch [190/875], Loss: 0.7976\n","Epoch [7/10], Batch [200/875], Loss: 0.8508\n","Epoch [7/10], Batch [210/875], Loss: 0.8070\n","Epoch [7/10], Batch [220/875], Loss: 0.8351\n","Epoch [7/10], Batch [230/875], Loss: 0.8851\n","Epoch [7/10], Batch [240/875], Loss: 0.7789\n","Epoch [7/10], Batch [250/875], Loss: 0.8633\n","Epoch [7/10], Batch [260/875], Loss: 0.8008\n","Epoch [7/10], Batch [270/875], Loss: 0.8289\n","Epoch [7/10], Batch [280/875], Loss: 0.8101\n","Epoch [7/10], Batch [290/875], Loss: 0.7820\n","Epoch [7/10], Batch [300/875], Loss: 0.7945\n","Epoch [7/10], Batch [310/875], Loss: 0.8039\n","Epoch [7/10], Batch [320/875], Loss: 0.8258\n","Epoch [7/10], Batch [330/875], Loss: 0.8508\n","Epoch [7/10], Batch [340/875], Loss: 0.7976\n","Epoch [7/10], Batch [350/875], Loss: 0.7789\n","Epoch [7/10], Batch [360/875], Loss: 0.7914\n","Epoch [7/10], Batch [370/875], Loss: 0.7789\n","Epoch [7/10], Batch [380/875], Loss: 0.8570\n","Epoch [7/10], Batch [390/875], Loss: 0.7883\n","Epoch [7/10], Batch [400/875], Loss: 0.7664\n","Epoch [7/10], Batch [410/875], Loss: 0.8039\n","Epoch [7/10], Batch [420/875], Loss: 0.7976\n","Epoch [7/10], Batch [430/875], Loss: 0.8133\n","Epoch [7/10], Batch [440/875], Loss: 0.8383\n","Epoch [7/10], Batch [450/875], Loss: 0.8508\n","Epoch [7/10], Batch [460/875], Loss: 0.8101\n","Epoch [7/10], Batch [470/875], Loss: 0.8101\n","Epoch [7/10], Batch [480/875], Loss: 0.8476\n","Epoch [7/10], Batch [490/875], Loss: 0.8195\n","Epoch [7/10], Batch [500/875], Loss: 0.7976\n","Epoch [7/10], Batch [510/875], Loss: 0.8070\n","Epoch [7/10], Batch [520/875], Loss: 0.8164\n","Epoch [7/10], Batch [530/875], Loss: 0.7914\n","Epoch [7/10], Batch [540/875], Loss: 0.8414\n","Epoch [7/10], Batch [550/875], Loss: 0.8351\n","Epoch [7/10], Batch [560/875], Loss: 0.8070\n","Epoch [7/10], Batch [570/875], Loss: 0.8383\n","Epoch [7/10], Batch [580/875], Loss: 0.8320\n","Epoch [7/10], Batch [590/875], Loss: 0.7820\n","Epoch [7/10], Batch [600/875], Loss: 0.8508\n","Epoch [7/10], Batch [610/875], Loss: 0.7945\n","Epoch [7/10], Batch [620/875], Loss: 0.8133\n","Epoch [7/10], Batch [630/875], Loss: 0.7758\n","Epoch [7/10], Batch [640/875], Loss: 0.8133\n","Epoch [7/10], Batch [650/875], Loss: 0.8258\n","Epoch [7/10], Batch [660/875], Loss: 0.8226\n","Epoch [7/10], Batch [670/875], Loss: 0.7664\n","Epoch [7/10], Batch [680/875], Loss: 0.8101\n","Epoch [7/10], Batch [690/875], Loss: 0.8195\n","Epoch [7/10], Batch [700/875], Loss: 0.7789\n","Epoch [7/10], Batch [710/875], Loss: 0.7539\n","Epoch [7/10], Batch [720/875], Loss: 0.8664\n","Epoch [7/10], Batch [730/875], Loss: 0.7976\n","Epoch [7/10], Batch [740/875], Loss: 0.8195\n","Epoch [7/10], Batch [750/875], Loss: 0.7820\n","Epoch [7/10], Batch [760/875], Loss: 0.8414\n","Epoch [7/10], Batch [770/875], Loss: 0.8164\n","Epoch [7/10], Batch [780/875], Loss: 0.8008\n","Epoch [7/10], Batch [790/875], Loss: 0.7976\n","Epoch [7/10], Batch [800/875], Loss: 0.7976\n","Epoch [7/10], Batch [810/875], Loss: 0.8133\n","Epoch [7/10], Batch [820/875], Loss: 0.8445\n","Epoch [7/10], Batch [830/875], Loss: 0.8695\n","Epoch [7/10], Batch [840/875], Loss: 0.8258\n","Epoch [7/10], Batch [850/875], Loss: 0.8226\n","Epoch [7/10], Batch [860/875], Loss: 0.8476\n","Epoch [7/10], Batch [870/875], Loss: 0.8476\n","Epoch [8/10], Batch [10/875], Loss: 0.7601\n","Epoch [8/10], Batch [20/875], Loss: 0.8133\n","Epoch [8/10], Batch [30/875], Loss: 0.8164\n","Epoch [8/10], Batch [40/875], Loss: 0.8133\n","Epoch [8/10], Batch [50/875], Loss: 0.8320\n","Epoch [8/10], Batch [60/875], Loss: 0.7976\n","Epoch [8/10], Batch [70/875], Loss: 0.8070\n","Epoch [8/10], Batch [80/875], Loss: 0.8570\n","Epoch [8/10], Batch [90/875], Loss: 0.7914\n","Epoch [8/10], Batch [100/875], Loss: 0.8070\n","Epoch [8/10], Batch [110/875], Loss: 0.8289\n","Epoch [8/10], Batch [120/875], Loss: 0.7695\n","Epoch [8/10], Batch [130/875], Loss: 0.7414\n","Epoch [8/10], Batch [140/875], Loss: 0.8414\n","Epoch [8/10], Batch [150/875], Loss: 0.8195\n","Epoch [8/10], Batch [160/875], Loss: 0.7976\n","Epoch [8/10], Batch [170/875], Loss: 0.8226\n","Epoch [8/10], Batch [180/875], Loss: 0.8070\n","Epoch [8/10], Batch [190/875], Loss: 0.7914\n","Epoch [8/10], Batch [200/875], Loss: 0.8039\n","Epoch [8/10], Batch [210/875], Loss: 0.8414\n","Epoch [8/10], Batch [220/875], Loss: 0.7914\n","Epoch [8/10], Batch [230/875], Loss: 0.8039\n","Epoch [8/10], Batch [240/875], Loss: 0.8008\n","Epoch [8/10], Batch [250/875], Loss: 0.8351\n","Epoch [8/10], Batch [260/875], Loss: 0.7820\n","Epoch [8/10], Batch [270/875], Loss: 0.8695\n","Epoch [8/10], Batch [280/875], Loss: 0.8008\n","Epoch [8/10], Batch [290/875], Loss: 0.8039\n","Epoch [8/10], Batch [300/875], Loss: 0.8695\n","Epoch [8/10], Batch [310/875], Loss: 0.8414\n","Epoch [8/10], Batch [320/875], Loss: 0.7976\n","Epoch [8/10], Batch [330/875], Loss: 0.7945\n","Epoch [8/10], Batch [340/875], Loss: 0.8383\n","Epoch [8/10], Batch [350/875], Loss: 0.8164\n","Epoch [8/10], Batch [360/875], Loss: 0.7945\n","Epoch [8/10], Batch [370/875], Loss: 0.8101\n","Epoch [8/10], Batch [380/875], Loss: 0.8351\n","Epoch [8/10], Batch [390/875], Loss: 0.8226\n","Epoch [8/10], Batch [400/875], Loss: 0.8101\n","Epoch [8/10], Batch [410/875], Loss: 0.8164\n","Epoch [8/10], Batch [420/875], Loss: 0.8195\n","Epoch [8/10], Batch [430/875], Loss: 0.7976\n","Epoch [8/10], Batch [440/875], Loss: 0.8414\n","Epoch [8/10], Batch [450/875], Loss: 0.8320\n","Epoch [8/10], Batch [460/875], Loss: 0.7570\n","Epoch [8/10], Batch [470/875], Loss: 0.8445\n","Epoch [8/10], Batch [480/875], Loss: 0.7851\n","Epoch [8/10], Batch [490/875], Loss: 0.8289\n","Epoch [8/10], Batch [500/875], Loss: 0.8351\n","Epoch [8/10], Batch [510/875], Loss: 0.8195\n","Epoch [8/10], Batch [520/875], Loss: 0.7789\n","Epoch [8/10], Batch [530/875], Loss: 0.7820\n","Epoch [8/10], Batch [540/875], Loss: 0.8226\n","Epoch [8/10], Batch [550/875], Loss: 0.7976\n","Epoch [8/10], Batch [560/875], Loss: 0.7789\n","Epoch [8/10], Batch [570/875], Loss: 0.8570\n","Epoch [8/10], Batch [580/875], Loss: 0.8039\n","Epoch [8/10], Batch [590/875], Loss: 0.8351\n","Epoch [8/10], Batch [600/875], Loss: 0.7976\n","Epoch [8/10], Batch [610/875], Loss: 0.7883\n","Epoch [8/10], Batch [620/875], Loss: 0.8101\n","Epoch [8/10], Batch [630/875], Loss: 0.8226\n","Epoch [8/10], Batch [640/875], Loss: 0.8164\n","Epoch [8/10], Batch [650/875], Loss: 0.8383\n","Epoch [8/10], Batch [660/875], Loss: 0.8008\n","Epoch [8/10], Batch [670/875], Loss: 0.8414\n","Epoch [8/10], Batch [680/875], Loss: 0.8008\n","Epoch [8/10], Batch [690/875], Loss: 0.8164\n","Epoch [8/10], Batch [700/875], Loss: 0.8445\n","Epoch [8/10], Batch [710/875], Loss: 0.9039\n","Epoch [8/10], Batch [720/875], Loss: 0.8101\n","Epoch [8/10], Batch [730/875], Loss: 0.7976\n","Epoch [8/10], Batch [740/875], Loss: 0.8101\n","Epoch [8/10], Batch [750/875], Loss: 0.7976\n","Epoch [8/10], Batch [760/875], Loss: 0.7695\n","Epoch [8/10], Batch [770/875], Loss: 0.8445\n","Epoch [8/10], Batch [780/875], Loss: 0.7789\n","Epoch [8/10], Batch [790/875], Loss: 0.7851\n","Epoch [8/10], Batch [800/875], Loss: 0.8164\n","Epoch [8/10], Batch [810/875], Loss: 0.8164\n","Epoch [8/10], Batch [820/875], Loss: 0.8539\n","Epoch [8/10], Batch [830/875], Loss: 0.8289\n","Epoch [8/10], Batch [840/875], Loss: 0.8258\n","Epoch [8/10], Batch [850/875], Loss: 0.8320\n","Epoch [8/10], Batch [860/875], Loss: 0.8289\n","Epoch [8/10], Batch [870/875], Loss: 0.8351\n","Epoch [9/10], Batch [10/875], Loss: 0.8039\n","Epoch [9/10], Batch [20/875], Loss: 0.7570\n","Epoch [9/10], Batch [30/875], Loss: 0.8320\n","Epoch [9/10], Batch [40/875], Loss: 0.7883\n","Epoch [9/10], Batch [50/875], Loss: 0.8039\n","Epoch [9/10], Batch [60/875], Loss: 0.7976\n","Epoch [9/10], Batch [70/875], Loss: 0.8351\n","Epoch [9/10], Batch [80/875], Loss: 0.8101\n","Epoch [9/10], Batch [90/875], Loss: 0.7695\n","Epoch [9/10], Batch [100/875], Loss: 0.7914\n","Epoch [9/10], Batch [110/875], Loss: 0.8445\n","Epoch [9/10], Batch [120/875], Loss: 0.7914\n","Epoch [9/10], Batch [130/875], Loss: 0.7789\n","Epoch [9/10], Batch [140/875], Loss: 0.7945\n","Epoch [9/10], Batch [150/875], Loss: 0.8383\n","Epoch [9/10], Batch [160/875], Loss: 0.8101\n","Epoch [9/10], Batch [170/875], Loss: 0.8445\n","Epoch [9/10], Batch [180/875], Loss: 0.8039\n","Epoch [9/10], Batch [190/875], Loss: 0.7945\n","Epoch [9/10], Batch [200/875], Loss: 0.8226\n","Epoch [9/10], Batch [210/875], Loss: 0.7851\n","Epoch [9/10], Batch [220/875], Loss: 0.7945\n","Epoch [9/10], Batch [230/875], Loss: 0.8039\n","Epoch [9/10], Batch [240/875], Loss: 0.7851\n","Epoch [9/10], Batch [250/875], Loss: 0.7976\n","Epoch [9/10], Batch [260/875], Loss: 0.8133\n","Epoch [9/10], Batch [270/875], Loss: 0.7851\n","Epoch [9/10], Batch [280/875], Loss: 0.7820\n","Epoch [9/10], Batch [290/875], Loss: 0.7883\n","Epoch [9/10], Batch [300/875], Loss: 0.8164\n","Epoch [9/10], Batch [310/875], Loss: 0.8820\n","Epoch [9/10], Batch [320/875], Loss: 0.7820\n","Epoch [9/10], Batch [330/875], Loss: 0.7664\n","Epoch [9/10], Batch [340/875], Loss: 0.8258\n","Epoch [9/10], Batch [350/875], Loss: 0.8789\n","Epoch [9/10], Batch [360/875], Loss: 0.8414\n","Epoch [9/10], Batch [370/875], Loss: 0.7945\n","Epoch [9/10], Batch [380/875], Loss: 0.8508\n","Epoch [9/10], Batch [390/875], Loss: 0.8351\n","Epoch [9/10], Batch [400/875], Loss: 0.8320\n","Epoch [9/10], Batch [410/875], Loss: 0.8258\n","Epoch [9/10], Batch [420/875], Loss: 0.8164\n","Epoch [9/10], Batch [430/875], Loss: 0.8164\n","Epoch [9/10], Batch [440/875], Loss: 0.8070\n","Epoch [9/10], Batch [450/875], Loss: 0.7883\n","Epoch [9/10], Batch [460/875], Loss: 0.8851\n","Epoch [9/10], Batch [470/875], Loss: 0.7976\n","Epoch [9/10], Batch [480/875], Loss: 0.7945\n","Epoch [9/10], Batch [490/875], Loss: 0.8195\n","Epoch [9/10], Batch [500/875], Loss: 0.8039\n","Epoch [9/10], Batch [510/875], Loss: 0.7883\n","Epoch [9/10], Batch [520/875], Loss: 0.8258\n","Epoch [9/10], Batch [530/875], Loss: 0.8195\n","Epoch [9/10], Batch [540/875], Loss: 0.8383\n","Epoch [9/10], Batch [550/875], Loss: 0.8195\n","Epoch [9/10], Batch [560/875], Loss: 0.8070\n","Epoch [9/10], Batch [570/875], Loss: 0.8133\n","Epoch [9/10], Batch [580/875], Loss: 0.7883\n","Epoch [9/10], Batch [590/875], Loss: 0.8570\n","Epoch [9/10], Batch [600/875], Loss: 0.8101\n","Epoch [9/10], Batch [610/875], Loss: 0.8289\n","Epoch [9/10], Batch [620/875], Loss: 0.8039\n","Epoch [9/10], Batch [630/875], Loss: 0.8320\n","Epoch [9/10], Batch [640/875], Loss: 0.8039\n","Epoch [9/10], Batch [650/875], Loss: 0.8445\n","Epoch [9/10], Batch [660/875], Loss: 0.7851\n","Epoch [9/10], Batch [670/875], Loss: 0.8508\n","Epoch [9/10], Batch [680/875], Loss: 0.8383\n","Epoch [9/10], Batch [690/875], Loss: 0.7820\n","Epoch [9/10], Batch [700/875], Loss: 0.8570\n","Epoch [9/10], Batch [710/875], Loss: 0.8008\n","Epoch [9/10], Batch [720/875], Loss: 0.8351\n","Epoch [9/10], Batch [730/875], Loss: 0.8164\n","Epoch [9/10], Batch [740/875], Loss: 0.8476\n","Epoch [9/10], Batch [750/875], Loss: 0.8008\n","Epoch [9/10], Batch [760/875], Loss: 0.8351\n","Epoch [9/10], Batch [770/875], Loss: 0.8070\n","Epoch [9/10], Batch [780/875], Loss: 0.8258\n","Epoch [9/10], Batch [790/875], Loss: 0.8164\n","Epoch [9/10], Batch [800/875], Loss: 0.8039\n","Epoch [9/10], Batch [810/875], Loss: 0.8101\n","Epoch [9/10], Batch [820/875], Loss: 0.7945\n","Epoch [9/10], Batch [830/875], Loss: 0.8476\n","Epoch [9/10], Batch [840/875], Loss: 0.8445\n","Epoch [9/10], Batch [850/875], Loss: 0.8133\n","Epoch [9/10], Batch [860/875], Loss: 0.7883\n","Epoch [9/10], Batch [870/875], Loss: 0.8164\n","Epoch [10/10], Batch [10/875], Loss: 0.8351\n","Epoch [10/10], Batch [20/875], Loss: 0.8476\n","Epoch [10/10], Batch [30/875], Loss: 0.8070\n","Epoch [10/10], Batch [40/875], Loss: 0.8070\n","Epoch [10/10], Batch [50/875], Loss: 0.7945\n","Epoch [10/10], Batch [60/875], Loss: 0.8070\n","Epoch [10/10], Batch [70/875], Loss: 0.8133\n","Epoch [10/10], Batch [80/875], Loss: 0.8101\n","Epoch [10/10], Batch [90/875], Loss: 0.8226\n","Epoch [10/10], Batch [100/875], Loss: 0.8351\n","Epoch [10/10], Batch [110/875], Loss: 0.7914\n","Epoch [10/10], Batch [120/875], Loss: 0.8601\n","Epoch [10/10], Batch [130/875], Loss: 0.8101\n","Epoch [10/10], Batch [140/875], Loss: 0.7945\n","Epoch [10/10], Batch [150/875], Loss: 0.8226\n","Epoch [10/10], Batch [160/875], Loss: 0.7726\n","Epoch [10/10], Batch [170/875], Loss: 0.8289\n","Epoch [10/10], Batch [180/875], Loss: 0.7726\n","Epoch [10/10], Batch [190/875], Loss: 0.7758\n","Epoch [10/10], Batch [200/875], Loss: 0.8539\n","Epoch [10/10], Batch [210/875], Loss: 0.8039\n","Epoch [10/10], Batch [220/875], Loss: 0.7851\n","Epoch [10/10], Batch [230/875], Loss: 0.8226\n","Epoch [10/10], Batch [240/875], Loss: 0.7914\n","Epoch [10/10], Batch [250/875], Loss: 0.8070\n","Epoch [10/10], Batch [260/875], Loss: 0.8101\n","Epoch [10/10], Batch [270/875], Loss: 0.8664\n","Epoch [10/10], Batch [280/875], Loss: 0.7633\n","Epoch [10/10], Batch [290/875], Loss: 0.7883\n","Epoch [10/10], Batch [300/875], Loss: 0.7789\n","Epoch [10/10], Batch [310/875], Loss: 0.8320\n","Epoch [10/10], Batch [320/875], Loss: 0.8476\n","Epoch [10/10], Batch [330/875], Loss: 0.8320\n","Epoch [10/10], Batch [340/875], Loss: 0.7820\n","Epoch [10/10], Batch [350/875], Loss: 0.8195\n","Epoch [10/10], Batch [360/875], Loss: 0.8351\n","Epoch [10/10], Batch [370/875], Loss: 0.8289\n","Epoch [10/10], Batch [380/875], Loss: 0.8070\n","Epoch [10/10], Batch [390/875], Loss: 0.7883\n","Epoch [10/10], Batch [400/875], Loss: 0.8570\n","Epoch [10/10], Batch [410/875], Loss: 0.8351\n","Epoch [10/10], Batch [420/875], Loss: 0.8070\n","Epoch [10/10], Batch [430/875], Loss: 0.8351\n","Epoch [10/10], Batch [440/875], Loss: 0.7695\n","Epoch [10/10], Batch [450/875], Loss: 0.8039\n","Epoch [10/10], Batch [460/875], Loss: 0.7945\n","Epoch [10/10], Batch [470/875], Loss: 0.8101\n","Epoch [10/10], Batch [480/875], Loss: 0.8101\n","Epoch [10/10], Batch [490/875], Loss: 0.7945\n","Epoch [10/10], Batch [500/875], Loss: 0.7883\n","Epoch [10/10], Batch [510/875], Loss: 0.7914\n","Epoch [10/10], Batch [520/875], Loss: 0.7758\n","Epoch [10/10], Batch [530/875], Loss: 0.8258\n","Epoch [10/10], Batch [540/875], Loss: 0.8039\n","Epoch [10/10], Batch [550/875], Loss: 0.8570\n","Epoch [10/10], Batch [560/875], Loss: 0.8289\n","Epoch [10/10], Batch [570/875], Loss: 0.8351\n","Epoch [10/10], Batch [580/875], Loss: 0.7851\n","Epoch [10/10], Batch [590/875], Loss: 0.7820\n","Epoch [10/10], Batch [600/875], Loss: 0.8383\n","Epoch [10/10], Batch [610/875], Loss: 0.7945\n","Epoch [10/10], Batch [620/875], Loss: 0.8758\n","Epoch [10/10], Batch [630/875], Loss: 0.8445\n","Epoch [10/10], Batch [640/875], Loss: 0.8258\n","Epoch [10/10], Batch [650/875], Loss: 0.8039\n","Epoch [10/10], Batch [660/875], Loss: 0.7726\n","Epoch [10/10], Batch [670/875], Loss: 0.7695\n","Epoch [10/10], Batch [680/875], Loss: 0.8164\n","Epoch [10/10], Batch [690/875], Loss: 0.7476\n","Epoch [10/10], Batch [700/875], Loss: 0.8914\n","Epoch [10/10], Batch [710/875], Loss: 0.8539\n","Epoch [10/10], Batch [720/875], Loss: 0.8133\n","Epoch [10/10], Batch [730/875], Loss: 0.8445\n","Epoch [10/10], Batch [740/875], Loss: 0.8539\n","Epoch [10/10], Batch [750/875], Loss: 0.8039\n","Epoch [10/10], Batch [760/875], Loss: 0.7945\n","Epoch [10/10], Batch [770/875], Loss: 0.8570\n","Epoch [10/10], Batch [780/875], Loss: 0.8633\n","Epoch [10/10], Batch [790/875], Loss: 0.7914\n","Epoch [10/10], Batch [800/875], Loss: 0.8414\n","Epoch [10/10], Batch [810/875], Loss: 0.8070\n","Epoch [10/10], Batch [820/875], Loss: 0.8226\n","Epoch [10/10], Batch [830/875], Loss: 0.8226\n","Epoch [10/10], Batch [840/875], Loss: 0.8258\n","Epoch [10/10], Batch [850/875], Loss: 0.7820\n","Epoch [10/10], Batch [860/875], Loss: 0.7976\n","Epoch [10/10], Batch [870/875], Loss: 0.8320\n","Finished Training\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Assuming you have your data in the form of BERT and Doc2Vec embeddings, and labels\n","# Replace these tensors with your actual data\n","bert_data = torch.tensor(roberta_embeddings_list)  # Replace with your BERT data\n","print(bert_data.shape)\n","doc2vec_data = torch.tensor(doc2vec_embeddings_list)  # Replace with your Doc2Vec data\n","print(doc2vec_data.shape)\n","labels = torch.tensor(labels_list)  # Replace with your labels\n","print(labels.shape)\n","\n","# Hyperparameters\n","bert_input_size = 768  # Example input size for BERT embeddings\n","doc2vec_input_size = 768  # Example input size for Doc2Vec embeddings\n","hidden_size = 768\n","num_classes = 2\n","learning_rate = 0.001\n","batch_size = 32\n","epochs = 10\n","\n","# Create DataLoader\n","dataset = TensorDataset(bert_data, doc2vec_data, labels)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize the model\n","model = TwoInputClassifier(bert_input_size, doc2vec_input_size, hidden_size, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(dataloader, 0):\n","        # Get the inputs; data is a list of [bert_input, doc2vec_input, labels]\n","        bert_input, doc2vec_input, labels = data\n","        \n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward + backward + optimize\n","        outputs = model(bert_input, doc2vec_input)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # Print every 10 mini-batches\n","            print(f\"Epoch [{epoch + 1}/{epochs}], \"\n","                  f\"Batch [{i + 1}/{len(dataloader)}], \"\n","                  f\"Loss: {running_loss / 10:.4f}\")\n","            running_loss = 0.0\n","\n","print(\"Finished Training\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Predict Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.228005Z","iopub.status.idle":"2023-11-27T05:02:18.228313Z","shell.execute_reply":"2023-11-27T05:02:18.228175Z","shell.execute_reply.started":"2023-11-27T05:02:18.228158Z"},"trusted":true},"outputs":[],"source":["# Tokenize and obtain embeddings for each text entry\n","test_roberta_embeddings_list = []\n","\n","test_text_list = test_data['text'].to_list()\n","for text in test_text_list:\n","    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n","    with torch.no_grad():\n","        output = roberta(**encoded_input)\n","    last_hidden_states = output.last_hidden_state\n","    # For simplicity, let's use the [CLS] token representation as the sentence embedding\n","    cls_embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n","    roberta_embeddings_list.append(cls_embedding)\n","# Step 3: Graph Construction\n","    \n","unlabeled_features = torch.tensor(test_roberta_embeddings_list, dtype=torch.float32)\n","\n","# Step 2: Set model to evaluation mode\n","model.eval()\n","\n","# Step 3: Predict logits for unlabeled nodes\n","with torch.no_grad():\n","    logits_unlabeled = model(dgl_G, unlabeled_features)\n","    logits_unlabeled = logits_unlabeled.squeeze(1)  # Squeeze unnecessary dimensions\n","\n","# Step 4: Apply threshold (e.g., sigmoid) for binary predictions\n","preds_val = (torch.sigmoid(logits_unlabeled) > 0.5).float()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Performance and Create Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:02:18.229309Z","iopub.status.idle":"2023-11-27T05:02:18.229621Z","shell.execute_reply":"2023-11-27T05:02:18.229476Z","shell.execute_reply.started":"2023-11-27T05:02:18.229460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC train: 0.9930497268286299\n","ROC AUC val: 0.5104081652548534\n"]}],"source":["if not is_submission:\n","    print('ROC AUC val:', roc_auc_score(test_data.generated, preds_val))\n","else:\n","    submission = pd.DataFrame({'id':test_data[\"id\"], 'generated':predictions})\n","    submission_path = r\"/kaggle/working/submission.csv\"\n","    submission.to_csv(submission_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6888007,"sourceId":61542,"sourceType":"competition"},{"datasetId":4005256,"sourceId":6977472,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
